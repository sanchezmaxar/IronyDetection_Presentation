# -*- coding: utf-8 -*-
"""Experimento1_porPalabras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C-hj_TaaRO429jmV73omDwLRoL0LXRcw

# Irony detection using Bi-LSTM

```
En este experimento se probó usar las palabras como elementos únicos de este modo detectar por ejemplo si usar la palabra hola o 'en serio' se pueda distinguir si es una oración irónica o no.

```
Primero  se añadiran las bibliotecas necesarias que se usaran en el código de este notebook.
"""

import pickle
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.python.keras.layers import Input, LSTM, Bidirectional, Dense, Embedding, Dropout
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

"""Después se leerán los vectores que se prepocesaron usando el código que dice preprocesamiento.py"""

def LoadData(archPkl,maxlen,max_features,base):
  # Load data
  (x_train, y_train),(x_test,y_test),basura= pickle.load(open(base+archPkl,'rb'))
  # Reverse sequences
  x_train = [x[::-1] for x in x_train]
  x_test = [x[::-1] for x in x_test]

  # Pad sequences
  x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
  x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
  return (x_train, y_train),(x_test,y_test)

"""Después se realizará la arquitectura de la red neuronal."""

def make_model(max_features,batch_size=None):
    source = Input(shape=(maxlen,), batch_size=batch_size, dtype=tf.int32, name='Input')
    embedding = Embedding(input_dim=max_features, output_dim=128, name='Embedding')(source)
    lstm = Bidirectional(LSTM(200, name = 'LSTM'), name='Bidirectional')(embedding)
    dropout=Dropout(0.5,name="Dropout")(lstm)
    predicted_var = Dense(1, activation='sigmoid', name='Output')(dropout)
    model = tf.keras.Model(inputs=[source], outputs=[predicted_var])
    model.compile(
            optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),
            loss='binary_crossentropy',
            metrics=['acc'])
    return model

"""Ahora se entrenará el modelo usando los TPU que se puden usar gracias a Google."""

import os
import time

def trainModel(training_model,x_train,y_train,x_test,y_test):
  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  tf.logging.set_verbosity(tf.logging.INFO)
  # this line is to don't print warnings and info that tf use by default
  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
  tf.logging.set_verbosity(tf.logging.ERROR)
  
  tpu_model = tf.contrib.tpu.keras_to_tpu_model(
      training_model,
      strategy=tf.contrib.tpu.TPUDistributionStrategy(
          tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))

  tpu_model.summary()

  start_time = time.time()

  history = tpu_model.fit(x_train, y_train,
                      epochs=5,
                      batch_size=32 * 8,
                      validation_data=[x_test,y_test])
  tpu_model.save_weights('./tpu_model.h5', overwrite=True)
  
  print("--- %s seconds ---" % (time.time() - start_time))
  return tpu_model

"""Por último se evalua el módelo y se imprime la gráfica ROC"""

from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score,f1_score,roc_curve
import numpy as np
import matplotlib.pyplot as plt

def evaluate(x_test,y_test,inferencing):
  
  inferencing_model.load_weights('./tpu_model.h5')
  try:
    files.download('./tpu_model.h5')
  except:
    pass
  
  y_pred=inferencing_model.predict(x_test,verbose=1,batch_size=32)
  fpr, tpr, thresholds = roc_curve(y_test.astype(float),y_pred )
  y_pred=np.array([int(i) for i in map(np.rint,y_pred)])

  y_test=np.array([int(i) for i in map(np.rint,y_test)])
  # 	for i in range(15):
  # 		print("Test: ",y_test[i])
  # 	# print(y_test)
  print("accuracy ",accuracy_score(y_test,y_pred))
  print("Matriz de confusión ",confusion_matrix(y_test,y_pred))
  print("Recall ",recall_score(y_test,y_pred,average='binary'))
  print("Precision ",precision_score(y_test,y_pred,average='binary'))
  print("F1-score ",f1_score(y_test,y_pred,average='binary'))
  return fpr,tpr

"""Ahora necesitamos aplicarlo para cada uno de nuestros vectores ya preprocesados."""

base="/content/drive/My Drive/Colab Notebooks/Experimento1/"
ListVect=["vectores{0}.pkl".format(i) for i in range(5)]
# Number of words to consider as features
max_features = 200000
# Cut texts after this number of words (among top max_features most common words)
maxlen = 100
model=make_model(max_features)
for vects in ListVect:
  print("-------------Set {0}-------------".format(vects))
  (x_train, y_train),(x_test,y_test)=LoadData(vects,maxlen,max_features,base)  
  tpu_model=trainModel(model,x_train,y_train,x_test,y_test)
  fpr,tpr=evaluate(x_test,y_test,tpu_model)
  plt.figure(1)
  plt.plot([0, 1], [0, 1], 'k--')
  plt.plot(fpr, tpr, label='1')
  plt.show()